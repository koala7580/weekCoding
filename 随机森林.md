## 一、个体与集成

集成学习通过构建并结合多个学习器来完成学习任务；

个体学习器通常由一个现有的学习算法从训练数据产生，如C4.5决策树算法、Bp神经网络等。个体学习器至少不差于弱学习器。

根据个体学习器的生成方式，目前的集成学习方法大致分为两类：

- 个体学习器间存在强依赖关系，必须串行生成的序列化方法——代表是Boosting；
- 个体学习器间不存在强依赖关系，可同时生成的并行化方法——代表是Bagging和“随机森林”（random forest）。

## 二、Boosting

Boosting是一族可将弱学习器提升为强学习器的算法。代表算法：Adaboost算法，基于“加性模型”，即基学习器的线性组合来最小化指数损失函数。

![1551363878928](D:\img\AdaBoost.png)

采用“重采样法”，可获得“重启动”机会以避免训练过程过早停止。

从**偏差-方差分解**的角度看，Boosting主要关注**降低偏差**，因此Boosting能基于泛化性能相当弱的学习器构造出很强的集成。

## 三、Bagging与随机森林

问题：为得到泛化性能强的集成，集成中的个体学习器应尽可能相互独立。一种是对训练样本采样得到若干不同的子集在从中训练基学习器；为了获得好的集成，希望个体学习器不能太差，即采样子集不能完全不同。因此考虑使用相互有交叠的采样子集。

### 1. Bagging

Bagging是并行式集成学习的著名代表。直接基于自助采样法（bootstrap sampling）（我理解的是有放回采样），进行采样。这样可以采样出T个含m个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器进行结合，这就是Bagging的基本流程。

在对预测输出进行结合时，Bagging通常对**分类任务**使用**简单的投票法**，对**回归任务**使用**简单平均法**。

![1551365403362](D:\img\Bagging.png)

训练一个Bagging集成与直接使用基学习算法训练一个学习器的复杂度同阶，说明Bagging是一个很高效的集成学习算法，另外，与标准AdaBoost只适用于二分类任务不同，Bagging能不经修改的用于多分类、回归等任务。自助采样给Bagging带来了另一个优点：由于每个基学习器只使用了初始训练集中约63.2%的样本，剩下的样本可用作验证集来对泛化性能进行“外包估计”。

从**偏差-方差分解**的角度看，Bagging主要关注降低方差，因此他在不剪枝决策树、神经网络等**易受样本扰动**的学习器上效用更为明显。

### 2. 随机森林 (Random Forest，RF)

RF是Bagging的一个扩展变体。RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。

（决策树相关内容，参见《统计机器学习》第5章，[决策树笔记]( )）

**（1）定义**

​	用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类。
```
（1）随机森林既可以用于分类，也可以用于回归。
（2）它是一种降维手段，用于处理缺失值和异常值。
（3）它是集成学习的重要方法。
```
**（2）基本原理**

​	随机森林由LeoBreiman（2001）提出，它通过自助法（bootstrap）重采样技术，从原始训练样本集N中有放回地重复随机抽取k个样本生成新的训练样本集合，然后根据自助样本集生成k个分类树组成随机森林，新数据的分类结果按分类树投票多少形成的分数而定。其实质是对决策树算法的一种改进，将多个决策树合并在一起，每棵树的建立依赖于一个独立抽取的样品，森林中的每棵树具有相同的分布，分类误差取决于每一棵树的分类能力和它们之间的相关性。特征选择采用随机的方法去分裂每一个节点，然后比较不同情况下产生的误差。能够检测到的内在估计误差、分类能力和相关性决定选择特征的数目。单棵树的分类能力可能很小，但在随机产生大量的决策树后，一个测试样品可以通过每一棵树的分类结果经统计后选择最可能的分类。

​	具体：传统决策树在选择划分属性时是在当前节点的属性集合（假设有d个属性）中选择一个最优属性；而在RF中，对基决策树的每个节点，从该节点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分，这里的参数k控制了随机性的引入程度：k=d，基决策树的构建与传统决策树相同；k=1则是随机选择一个属性用于划分。一般情况下，推荐k=log2d。

**（3）RF的优点** 

```
a. 在数据集上表现良好，两个随机性的引入，使得随机森林不容易陷入过拟合

b. 在当前的很多数据集上，相对其他算法有着很大的优势，两个随机性的引入，使得随机森林具有很好的抗噪声能力

c. 它能够处理很高维度（feature很多）的数据，并且不用做特征选择，对数据集的适应能力强：既能处理离散型数据，也能处理连续型数据，数据集无需规范化

d. 可生成一个Proximities=（pij）矩阵，用于度量样本之间的相似性： pij=aij/N, aij表示样本i和j出现在随机森林中同一个叶子结点的次数，N随机森林中树的颗数

e. 在创建随机森林的时候，对generlization error使用的是无偏估计

f. 训练速度快，可以得到变量重要性排序（两种：基于OOB误分率的增加量和基于分裂时的GINI下降量

g. 在训练过程中，能够检测到feature间的互相影响

h. 容易做成并行化方法

i. 实现比较简单随机森林简单、容易实现、计算开销小，在很多现实任务中展现出强大的性能。RF中基学习器的多样性不仅来自样本扰动，还来自属性扰动，这就使得最终集成的泛化性能可以通过个体学习器之间的差异度的增加而进一步提升。
```

**（4）缺点**
```
当我们需要推断超出范围的独立变量或非独立变量，随机森林做得并不好，我们最好使用如 MARS 那样的算法。

随机森林算法在训练和预测时都比较慢。

如果需要区分的类别十分多，随机森林的表现并不会很好。
```
**（5）应用范围**

主要应用于回归和分类。

链接中有参考案例。

### 参考链接

[随机森林原理](https://www.jianshu.com/p/57e862d695f2)
[随机森林算法详解](https://my.oschina.net/u/3121322/blog/806651)
[从决策树到随机森林](https://www.jiqizhixin.com/articles/2017-07-31-3)

[sklearn参数学习](https://www.jianshu.com/p/516f009c0875)


## 四、结合策略

学习器结合可以带来的好处：

- （1）从**统计的方面**来看，由于学习任务的假设空间很大，可能有多个假设在训练集上达到同等性能，单学习器可能因误选导致泛化性能不佳，结合多个学习器则会减少这一风险；

- （2）从**计算的方面**来看，学习算法往往会陷入局部极小，多次运行之后进行结合可以降低陷入糟糕局部极小点的风险；

- （3）从**表示的方面**来看，某些学习任务的真是假设可能不在当前学习算法所考虑的假设空间中，若使用单学习器则肯定无效，而通过结合多个学习器，有可能学得更好的近似。

**1. 平均法**

- 简单平均法：$H(x)=\frac{1}{T}\sum_{i=1}^{T}h_i(x)$

- 加权平均法： $H(x)=\sum_{i=1}^{T}w_ih_i(x)$

其中$w_i​$是个体学习器$h_i​$的权重，通常要求$w_i\geq0,\sum_{i=1}^{T}w_i=1​$

一般而言，在个体学习器性能相差较大时，宜使用加权平均法，而在个体学习器性能相近时，宜使用即按但平均法。

**2. 投票法**

- 绝对多数投票法：$H(x)=\begin{cases} c_j,\quad if \quad \sum_{i=1}^Th_i^j(x)>0.5 \sum_{k=1}^N \sum_{i=1}^Th_i^k(x) \\\\ reject, \quad otherwise.\end{cases}​$  

  即若某标记得票过半数，则预测为该标记；否则拒绝预测。

- 相对多数投票法：$H(x)=c_{argmax_j{\sum_{i=1}^Th_i^j(x)}} ​$ 

  即预测为得票最多的标记，若同时有多个标记获得最高票，则从中随机选择一个

- 加权投票法：$H(x)=c_{argmax_j{\sum_{i=1}^Tw_ih_i^j(x)}} $ 

  与加权平均法类似，$w_i$是$h_i$的权重，通常$w_i\geq0,\sum_{i=1}^{T}w_i=1$

在标准的绝对多数投票法提供了“拒绝预测”选项，这在可靠性要求较高的学习任务中是一个很好的机制，单若学习任务要求必须提供预测结果，则绝对多数投票法将退化为相对多数投票法。因此，在不允许拒绝预测的任务中，绝对多数、相对多数投票法统称为“多数投票法”。

- 类标记：$h_i^j\in \lbrace 0,1 \rbrace​$.使用类标记的投票称为**硬投票**。
- 类概率：$h_i^j\in [0,1]$.使用类概率的投票称为**软投票**。

**3.学习法**

当训练数据很多时，一种更为强大的结合策略是使用“学习法”，即通过另一个学习器来进行结合。Stacking是学习法的典型代表。称个体学习器为初级学习器，用于结合的学习器称为次级学习器或元学习器。Stacking先从初始数据集训练出初级学习器，然后“生成”一个新数据集用于训练次级学习器。

![1551403183245](D:\img\Stacking.png)

次级学习器的输入属性表示和次级学习算法对Stacking集成的泛化性能有很大影响。将初级学习器的输出类概率作为次基学习器的输入属性，用多响应线性回归（Multi-response Linear Regression，MLR）作为次级学习算法效果较好，在MLR中使用不同的属性集更佳。

```
MLR是基于线性回归的分类器，他对每个类进行线性回归，属于该类的训练阳历所对应的输出被置为1，其他类置为0；测试示例将被分给输出值最大的类。
```

贝叶斯模型平均（Bayes Model Averaging，BMA）基于后验概率来为不同模型赋予权重，可视为加权平均法的一种特殊实现。理论上，若数据生成模型恰在当前考虑的模型中，且数据噪声很少，则BMA不差于Stacking；然而现实中无法确保数据生成模型一定在当前考虑的模型中，甚至可能难以用当前考虑的模型来近似。因此，**Stacking通常由于BMA，因为其鲁棒性比BMA更好，而且BMA对模型近似误差非常敏感**。

## 五、多样性

### 1. 误差-分歧分解

用个体学习器h1,h2,...hT通过加权平均法结合产生的集成来完成回归学习任务 f：R^d->R。对示例$x​$,定义学习器$h_i​$的“分歧”（ambiguity）为：

$$A(h_i|x)=(h_i(x)-H(x))^2$$

则集成的“分歧”是：

$$\tilde{A}(h|x)=\sum_{i=1}^Tw_iA(h_i|x) \\\  =\sum_{i=1}^Tw_i(h_i(x)-H(x))^2$$

通过**《西瓜书》**8.29 ~ 8.35 公式可推导得 $E=\tilde{E}-\tilde{A}$ (8.36)

式（8.36）这个漂亮的式子明确提示出：**个体学习器准确性越高、多样性越大，则集成越好**，称为**“误差-分歧分解”**。

### 2. 多样性度量（diversity measure）

多样性度量是用于度量集成中个体分类器的多样性，即**估算个体学习器的多样化程度**。典型的做法是考虑个体分类器的两两相似/不相似性。

常见的多样性度量（太懒了，不想敲字了，贴图 →_→ ）

![1551409142896](D:\img\常见多样性度量.png)

则k=0,k通常为非负值，仅在hi与hj达成一致的概率甚至低于偶然性的情况下取负值。

### 3. 多样性增强

一般思路：在学习过程中引入随机性，常见做法主要是对数据样本、输入属性、输出表示、算法参数进行扰动。

- 数据样本扰动：通常基于采样法，简单高效。但是对于一些基学习器对数据样本的扰动不敏感，例如线性学习器、支持向量机、朴素贝叶斯、K近邻等，对此类进行集成需使用输入属性扰动等其他机制。
- 输入属性扰动：训练样本通常由一组属性描述，不同“子空间”（subspace，属性子集）提供了观察数据的不同视角。（（突然感叹用词的严谨性！））从不同子空间训练出的个体学习器必然有所不同。
 ![1551409656643](D:\img\随机子空间算法.png)

- 输出表示扰动：对输出表示进行操纵以增强多样性，可**对训练样本的标记稍作变动**，如“翻转法”随机改变一些训练样本的标记，也可**对输出表示进行转化**，如“输出调制法”将分类输出转化为回归输出后构建个体学习器；还可**将原任务拆解为多个可同时求解的子任务**，如ECOC法利用纠错参数码将多类任务拆解为一系列二分类任务来训练基学习器。

- 算法参数扰动：随机设置不同的参数，往往可产生差别较大的个体学习器，如“负相关法”显式的通过正则化项来强制个体神经网络使用不同的参数。对于参数较少的算法，可将其学习过程中的某些环节用类似的方法替代，以达到扰动的目的。

**不同的多样性增强机制可同时使用**，如随机森林中同时使用了数据样本扰动和输入属性扰动。

《西瓜书》第八章完。

（ps.哇，还不如昨晚坚强的再熬一会学完了，上午效率好低哦 >_< ）

----







